Here is a detailed breakdown of **Part VII, Section C: Logging, Monitoring & Debugging**.

In a traditional server environment (like a Node.js container), debugging usually involves SSH-ing into a server and reading a log file, or attaching a debugger to a running process.

**Cloudflare is different.** Your code is running on thousands of servers in hundreds of cities simultaneously. You cannot "SSH into the edge." Therefore, Cloudflare provides a specific set of tools to visualize what is happening across this distributed network.

---

## 1. Real-time Logs with `wrangler tail`

This is the developer’s first line of defense. Because you cannot view the standard output (`stdout`) of a serverless function running in Tokyo while you are sitting in New York, Cloudflare uses a streaming service to pipe those logs back to your CLI.

### The Concept
When you write `console.log("Hello World")` in your Worker code, that text is captured by the edge server. `wrangler tail` opens a WebSocket connection to the Cloudflare network and listens for log events generated by your specific Worker.

### How to use it
In your terminal, inside your project directory:
```bash
npx wrangler tail
```

### Advanced Features
*   **Live Filtering:** If you have high traffic, `tail` will be overwhelmed. You can filter logs to find specific issues.
    ```bash
    # Only show logs where the status code is a server error (500)
    npx wrangler tail --status error

    # Only show logs from a specific IP address
    npx wrangler tail --ip 192.0.2.1

    # Search for specific text in the logs
    npx wrangler tail --search "critical_failure"
    ```
*   **Structured Logging:** Instead of logging strings, modern Workers development encourages logging JSON objects. `wrangler tail` renders these beautifully, allowing you to inspect complex state objects easily.

---

## 2. Workers Trace Events and Dashboard Log Explorer

`wrangler tail` is ephemeral—it only shows what is happening *right now* while the command is running. If an error occurred 10 minutes ago, `tail` won't help you. That is where **Trace Events** come in.

### Trace Events
A "Trace" captures the entire lifecycle of a request as it passes through your Worker. It records:
1.  **Request Info:** Method, URL, Headers, User Agent.
2.  **Trigger Time:** When the request hit the edge.
3.  **Execution:** How long the CPU ran, and how long the total request took (Wall time).
4.  **Exceptions:** Any uncaught JavaScript errors.
5.  **Logs:** Any `console.log` outputs attached to that specific request.

### The Dashboard Log Explorer
Located in the Cloudflare Dashboard under **Workers & Pages -> [Your Worker] -> Logs**.

*   This provides a UI to search through recent requests.
*   **The CF-Ray ID:** This is the most critical concept in Cloudflare debugging. Every single request that hits Cloudflare is assigned a unique `CF-Ray` header (e.g., `87a1b2c3d4e5f6-IAD`).
*   **Debugging Workflow:**
    1.  A user reports a bug.
    2.  You ask them for the `CF-Ray` ID (usually found in response headers).
    3.  You paste that ID into the Log Explorer.
    4.  You see exactly what happened during *that specific user's* request.

---

## 3. Source Maps for Production Error Debugging

### The Problem
To make Workers fast, your code is bundled and minified (spaces removed, variable names shortened to single letters) before upload.
If your code crashes, the stack trace might look like this:
`Error: null property access at worker.js:1:15402`

Line 1, column 15402 is useless to you.

### The Solution: Source Maps
Source maps are files that map the minified code running on the server back to your original source code (e.g., `index.ts` line 45).

### Enabling it
In your `wrangler.toml`:
```toml
upload_source_maps = true
```

When this is enabled, if your Worker throws an exception in production, the Cloudflare Dashboard (and `wrangler tail`) will use the source map to translate the error.
**Instead of `worker.js:1:15402`, you will see `src/handlers/auth.ts:45`.**

---

## 4. Integrating with Third-Party Observability (Logpush)

Cloudflare stores logs for a limited time (depending on your plan, this might be minutes or hours). For enterprise-grade applications, you need long-term retention and the ability to correlate Worker logs with your database logs or backend API logs.

### Cloudflare Logpush
Logpush is a service that automatically pushes logs from Cloudflare's Edge to your storage or observability provider.

1.  **Destinations:** It supports Amazon S3, Google Cloud Storage, Datadog, Splunk, Sumo Logic, New Relic, etc.
2.  **What it pushes:**
    *   **HTTP Requests:** Standard access logs (IP, status code, path).
    *   **Worker Events:** `console.log` output and exceptions.
    *   **Firewall Events:** logs of users blocked by your WAF.

**Why this matters:** You can set up alerts in Datadog (e.g., "Alert me if 500 errors exceed 1% of traffic") based on the data Logpush sends.

---

## 5. Analytics for WAF, DNS, and Workers

While "Logs" look at individual requests, "Analytics" look at aggregate trends.

### Workers Metrics
*   **Requests:** Total volume of hits.
*   **CPU Time:** How much computational work your worker is doing. (Crucial for billing and optimization).
    *   *Note:* If you exceed CPU limits (e.g., 10ms on Free, 50ms on Bundled), the worker is killed. Analytics helps you spot if you are getting close to this limit.
*   **Duration:** How long the user waited (latency).
*   **Failure Rate:** The percentage of requests returning 5xx errors or throwing exceptions.

### WAF (Security) Analytics
This view tells you **who is attacking you**.
*   It visualizes blocked requests.
*   It breaks traffic down by country, user agent, and specific WAF rule (e.g., "Blocked by OWASP SQL Injection Rule").

### DNS Analytics
Shows the raw DNS queries for your domain. This helps debug issues where users claim "the site doesn't exist" (NXDOMAIN errors) or verify if traffic is actually reaching Cloudflare.

---

## Summary Checklist for Debugging
If you are building a serious app on Cloudflare, your debugging workflow looks like this:

1.  **Development:** Use `wrangler dev` locally.
2.  **Staging/Live Debugging:** Use `wrangler tail` to see live traffic.
3.  **Production Error Hunting:** Use the **Dashboard Log Explorer** with a specific **Ray ID**.
4.  **Long-term Monitoring:** Configure **Logpush** to send data to Datadog/New Relic for alerting on error spikes.
