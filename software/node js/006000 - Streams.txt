Streams
    > Helps you write performant apps
    > `stream` module helps you work with streaming data

    > Pattern description
        > If you have a very big data and you want to perform any operations on it
          (read, write, transform)
        > Very big data means that its size is bigger than the memory size i.e. (50 GB)
        > You can not load it to the memory 
            > And if you somehow could it will take lots of time
        > The stream pattern solves the problem
            > It starts reading the file bit by bit
            > each bit (called chunk) is stored in a buffer zone 
            > when the buffer/bucket is full it is sent through the pipeline
            > this pipeline can be targeted any where
            > there are lots of events that lets you control the stream
                > `data` => chunk entered the pipeline
                > `end` => done
                > `error`
                > etc

    > Types of Streams
        > Readable stream
            > allows node js read data from source
            > Any class that extends `Readable`
            > i.e.
                > fs
                > process.stdin
        > Writable stream
            > allows you to write a stream to a file/destination
            > i.e.
                > fs
                > process.stdout
        > Duplex stream
            > A combination between both readable/writable Streams
            > Both sides are totally separated from each other
            > i.e.
                > Socket
        > Transform stream
            > Similar to duplex stream but readable side is connected to the writable side
            > i.e.
                > crypto.Cipher
                > stream.PassThrough

    > Stream events
        > Readable
            > data
                > chunk arrived from data source and ready to be consumed
            > end
                > emitted when there is no more data to be consumed
        > Writable
            > drain
                > writable can receive more data
            > finish
                > all data has been flushed to underlying system
        > You can use `pipe` instead of data and connect it to the next stream

    > Flowing modes of readable streams
        > pause, resume => converts between 2 modes
        > Paused Mode
            > default mode
            > can be switched to flowing and vise versa easily
            > once you listen to data will switch the state to flowing
        > Flowing Mode

    > You can write your own implementations
        > `new Readable`, `new Writable`, `new Duplex`, `new Transform`