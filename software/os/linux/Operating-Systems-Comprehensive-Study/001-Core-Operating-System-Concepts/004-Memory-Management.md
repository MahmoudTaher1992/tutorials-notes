Based on the Table of Contents you provided, here is a detailed breakdown of **Part I, Section D: Memory Management**.

This is one of the most critical responsibilities of an Operating System. The CPU is very fast, but it can only fetch instructions directly from the Main Memory (RAM). The OS must manage this limited resource efficiently to allow multiple programs to run simultaneously without crashing into each other.

---

### 1. Main Memory
This section covers the basic hardware/software interface regarding RAM.

*   **Logical vs. Physical Address Space**:
    *   **Physical Address**: This is the actual hardware address in the RAM chips (e.g., address `0x0000` to `0xFFFF`). The memory hardware only understands these addresses.
    *   **Logical (Virtual) Address**: This is the address generated by the CPU when a program is running. A program usually thinks it starts at address 0 and has access to a continuous block of memory.
    *   **The MMU (Memory Management Unit)**: This is a hardware device that translates Logical addresses to Physical addresses on the fly. This separation allows the OS to move programs around in physical RAM without breaking the code inside the program.

*   **Swapping**:
    *   Sometimes, you run more programs than your physical RAM can hold.
    *   **The Mechanism**: The OS takes a process that is currently idle, copies its entire memory intent to the hard disk (into a "swap file" or "partition"), and frees up that RAM for other active processes.
    *   **Roll-in/Roll-out**: When the user clicks on the swapped-out program, the OS pauses, reloads the data from the disk back into RAM, and resumes execution. This effectively expands the system's capacity but is significantly slower than using RAM.

---

### 2. Memory Allocation
This section explains *how* the OS assigns blocks of memory to processes.

*   **Contiguous Allocation**:
    *   **The Concept**: Each process is contained in a single, continuous section of memory. If a process needs 100MB, the OS must find a single 100MB hole.
    *   **The Problem**: This leads to **Fragmentation**.
        *   *External Fragmentation*: You have enough total free memory (e.g., 200MB total), but it is scattered in small chunks (50MB here, 20MB there), so you cannot fit a 100MB process anywhere.

*   **Paging (The Modern Standard)**:
    *   To solve fragmentation, the OS divides physical memory into fixed-sized blocks (frames) and breaks logical memory (the program) into blocks of the same size (pages).
    *   **How it works**: When a program needs to run, its *pages* are loaded into any available *frames* in memory. They do **not** have to be next to each other.
    *   **Page Table**: The OS keeps a detailed list (map) allowing it to say, "Page 1 of Chrome is in Frame 500," and "Page 2 of Chrome is in Frame 102."

*   **Segmentation**:
    *   Instead of fixed-size blocks (like paging), memory is divided based on logical units of the program (e.g., the function code, the data variables, the stack).
    *   Segments vary in size. This mirrors the user's view of memory more closely but suffers from external fragmentation similar to contiguous allocation. (Modern OSs often use a hybrid: Paging *over* Segmentation).

---

### 3. Virtual Memory
This introduces the illusion that the computer has more memory than it actually does.

*   **Demand Paging**:
    *   **Lazy Loading**: The OS does not load the *entire* program into RAM when the application starts. It only loads the specific pages currently needed to run.
    *   **Page Fault**: When the program tries to access a part of itself that hasn't been loaded into RAM yet, the hardware triggers a "trap" (interrupt). The OS catches this, pauses the program, fetches the missing page from the hard disk, puts it in RAM, and tells the program to try again.

*   **Copy-on-Write (COW)**:
    *   This is an optimization technique, commonly used when creating a new process (like the `fork()` system call in UNIX).
    *   Instead of duplicating all the memory for the new process immediately, the Parent and Child processes share the same pages in memory.
    *   If either process tries to **Write** (modify) data, only *then* does the OS create a copy of that specific page. This saves massive amounts of RAM and speeds up process creation.

*   **Page Replacement Algorithms**:
    *   When RAM is completely full, and a program creates a Page Fault (needs a new page from disk), the OS must decide which existing page in RAM to delete (evict) to make room.
    *   **FIFO (First-In-First-Out)**: Kick out the oldest page. (Problem: The oldest page might be the kernel or a graphic library used constantly).
    *   **LRU (Least Recently Used)**: Kick out the page that hasn't been used for the longest time. This is generally the best performer because if you haven't used data recently, you probably won't use it soon.
    *   **Optimal**: Theoretically, kick out the page that won't be used for the longest time in the *future*. (Impossible to implement perfectly because the OS cannot predict the future, but useful for benchmarking other algorithms).
