This outline describes the **lifecycle of a database query**â€”from the moment you type `SELECT` to the moment data appears on your screen. It covers how databases understand your code, how they decide the fastest way to run it, and how to measure performance.

Here is the detailed explanation of each section.

---

# 4. Query Execution Fundamentals

## 4.1 Query Processing Pipeline
This is the "assembly line" that transforms a human-readable SQL text string into machine execution.

### 4.1.1 Query parsing
The database must first check if the query makes sense linguistically.
*   **4.1.1.1 Lexical analysis:** The "Tokenizer." It breaks the string `SELECT * FROM users` into tokens: `[Keyword: SELECT]`, `[Symbol: *]`, `[Keyword: FROM]`, `[Identifier: users]`.
*   **4.1.1.2 Syntax validation:** Checks grammar rules. It ensures keywords are in the right order (e.g., rejecting `FROM users SELECT`) and parentheses are balanced.
*   **4.1.1.3 Parse tree generation:** Converts the linear text into a tree data structure that the computer can traverse. The root node might be "Query", branching into "Select List" and "From Clause".

### 4.1.2 Query rewriting
Before optimizing, the database simplifies the query logic. This step does not change the result, only the structure.
*   **4.1.2.1 View expansion:** If you query `SELECT * FROM premium_users` (where `premium_users` is a view), the rewriter replaces `premium_users` with the underlying SQL definition of that view.
*   **4.1.2.2 Subquery transformation:** Converting complex subqueries into standard Joins where possible.
    *   *Example:* Changing `WHERE id IN (SELECT user_id FROM orders)` into a `JOIN orders`.
*   **4.1.2.3 Predicate pushdown:** Moving filter conditions (`WHERE` clauses) as deep into the tree as possible.
    *   *Goal:* Filter data *before* joining or sorting to reduce the workload early.

### 4.1.3 Query optimization
The "Brain" of the database. It decides *how* to execute the query.
*   **4.1.3.1 Cost-based optimization (CBO):** The standard in modern databases. It generates multiple potential plans, assigns a "cost" (CPU + I/O units) to each based on statistics, and picks the cheapest one.
*   **4.1.3.2 Rule-based optimization (RBO):** An older method. It follows strict hard-coded priorities (e.g., "Always use an index if one exists," even if scanning the whole table is faster for that specific case).
*   **4.1.3.3 Heuristic optimization:** "Rules of thumb" used to prune the search space so the optimizer doesn't spend forever calculating. (e.g., "Don't try to reorder more than 10 joins").

### 4.1.4 Query execution
The "Muscle" of the database. It runs the plan generated by the optimizer.
*   **4.1.4.1 Execution engines:** The component that orchestrates the flow of data.
*   **4.1.4.2 Operator models:**
    *   **Volcano / Iterator:** The standard model. Each operator asks the one below it for the "next row." (Simple, but high CPU overhead due to millions of function calls).
    *   **Vectorized:** Operators process batches of rows (vectors) at a time. (Much faster, uses modern CPU SIMD instructions).
    *   **Compiled:** The query is compiled into native machine code (JIT) specifically for that query structure. (Very fast execution, but takes time to compile startup).

### 4.1.5 Result delivery
How the data is sent back to the client.
*   **4.1.5.1 Result set buffering:** The database generates *all* results and stores them in memory before sending the first one. (Safe, but high memory usage).
*   **4.1.5.2 Streaming results:** The database sends rows to the client as soon as they are found. (Fast "Time to First Byte," low memory usage).
*   **4.1.5.3 Cursor-based delivery:** The client requests a specific chunk (e.g., 100 rows), processes them, then asks for the next 100.

---

## 4.2 Execution Plans
The **Execution Plan** is the recipe the optimizer chose. Reading this is the primary skill in SQL performance tuning.

### 4.2.1 What is an execution plan?
A tree of operations (nodes) that the database will perform. It shows the order of operations, the methods used, and the estimated costs.

### 4.2.2 Logical vs. physical plans
*   **Logical:** Describes *what* to do (Algebra). Example: "Inner Join Table A and Table B."
*   **Physical:** Describes *how* to do it (Implementation). Example: "Hash Join Table A and Table B using Index Scan on A."

### 4.2.3 Plan operators
The specific actions in the recipe.
*   **4.2.3.1 Scan operators:**
    *   **Sequential/Full Scan:** Reading the table from top to bottom. (Best for small tables).
    *   **Index Scan:** Traversing a B-Tree to find a pointer. (Best for finding specific rows).
    *   **Bitmap Scan:** Combining multiple index results into a map of bits before visiting the table.
*   **4.2.3.2 Join operators:**
    *   **Nested Loop:** For every row in Table A, loop through Table B. (Good for small datasets).
    *   **Hash Join:** Load Table A into a Hash Map in memory, then scan Table B and check the map. (Good for large, unsorted datasets).
    *   **Merge Join:** Sort both tables, then "zipper" them together. (Best if data is already sorted).
*   **4.2.3.3 Aggregation:** Operators for `GROUP BY`, `SUM`, `AVG`. (Often use HashAgg or SortAgg).
*   **4.2.3.4 Sort:** Ordering data (`ORDER BY`). Expensive if it spills to disk.
*   **4.2.3.5 Set operators:** `UNION` (combine), `INTERSECT` (overlap), `EXCEPT` (A minus B).

### 4.2.4 Reading execution plans
*   **4.2.4.1 Plan tree structure:** Plans are read bottom-up. The leaves are the tables; the root is the final result.
*   **4.2.4.2 Cost estimates:** An arbitrary number representing effort. Useful for comparing two different plans, but not a unit of time (milliseconds).
*   **4.2.4.3 Row estimates (Cardinality):** The optimizer's *guess* at how many rows will pass through an operator.
*   **4.2.4.4 Actual vs. Estimated values:** The most critical metric. If the optimizer estimates 1 row but *actually* gets 1,000,000, it likely chose the wrong join type (e.g., Nested Loop instead of Hash), causing slowness.

### 4.2.5 Plan stability
*   **4.2.5.1 Plan changes over time:** A query was fast yesterday but slow today. Why? Usually because data volume changed, causing the optimizer to switch strategies (flapping).
*   **4.2.5.2 Parameter sensitivity (Parameter Sniffing):**
    *   Query: `SELECT * FROM orders WHERE country = ?`
    *   If the first run is `country = 'Luxembourg'` (small), the DB picks a plan optimized for small data.
    *   If the second run is `country = 'USA'` (huge), the DB reuses the "small" plan, which crashes performance.
*   **4.2.5.3 Plan regression detection:** Monitoring tools that alert you when a plan shape changes for the worse.
*   **4.2.5.4 Plan pinning/forcing:** Manually locking a specific execution plan (Query Hint or Plan Guide) so the optimizer is forbidden from changing it.

---

## 4.3 Query Statistics
These are the metrics gathered *after* execution to understand exactly where resources were spent.

### 4.3.1 Execution time breakdown
*   **4.3.1.1 Planning time:** How long the optimizer spent thinking. If this is high, the query is too complex (too many joins).
*   **4.3.1.2 Execution time:** The time spent actually retrieving data.
*   **4.3.1.3 Network time:** The time spent shipping the result packets to the client.

### 4.3.2 I/O statistics
Database performance is mostly about managing Input/Output.
*   **4.3.2.1 Buffer hits vs. disk reads:**
    *   **Buffer Hit:** The data was already in RAM (Cache). Super fast.
    *   **Disk Read:** The data had to be fetched from SSD/HDD. Slow.
    *   *Goal:* High Buffer Hit Ratio (e.g., > 99%).
*   **4.3.2.2 Pages read/written:** The number of 8kb (standard size) blocks accessed.
*   **4.3.2.3 Temporary file usage:** If a Sort or Hash Join is too big for RAM, the DB writes temporary files to disk. This kills performance.

### 4.3.3 Memory statistics
*   **4.3.3.1 Work memory usage:** RAM allocated per-operation.
*   **4.3.3.2 Sort memory:** RAM used specifically for `ORDER BY`. If exceeded -> spill to disk.
*   **4.3.3.3 Hash table memory:** RAM used to build the map for Hash Joins.

### 4.3.4 Row statistics
*   **4.3.4.1 Rows scanned:** Total rows the engine looked at.
*   **4.3.4.2 Rows filtered:** Rows examined but thrown away because they didn't match the `WHERE` clause.
    *   *Insight:* High "Rows Scanned" but low "Rows Returned" usually means you are missing an Index.
*   **4.3.4.3 Rows returned:** The final count sent to the user.