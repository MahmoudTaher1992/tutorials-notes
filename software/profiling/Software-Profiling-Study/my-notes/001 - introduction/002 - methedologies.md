# ğŸ› ï¸ Core Methodologies

## ğŸ” Instrumentation
*   **Definition:** Involves inserting code into the original code that sends data outside; the aim is to get insights.
*   **Risk:** It carries the risk of introducing an overhead that distorts the original results.
*   **ğŸ“‚ Types:**
    *   **âœï¸ Source Code Modification (Manual instrumentation)**
        *   You explicitly change your code to add the code that will send the data outside.
        *   **âœ… Pros:**
            *   You are in control.
        *   **âŒ Cons:**
            *   Takes time and effort.

    *   **ğŸ¤– Binary Injection (Automatic instrumentation)**
        *   An automated approach where the profiling tool modifies the compiled program at runtime or load time.
        *   **e.g.:**
            *   Wrapping.
            *   Code injection.
        *   **âœ… Pros:**
            *   Quick implementation.
        *   **âŒ Cons:**
            *   High overhead.
            *   Lots of noisy output data.

---

## â±ï¸ Sampling 
*   **Definition:** Instead of measuring stuff continuously, we measure at intervals/frequency.
*   **Mechanism:** The profiler estimates the execution time/result from the frequency measuring results.
*   **âœ… Pros:**
    *   This technique is used to decrease the overhead.
*   **âŒ Cons:**
    *   Some very fast functions/targets may be missed because of them outrunning the measuring code frequency (**Blind spot**).
## ğŸ“ Tracing
*   **Definition:** It is the process of regiestering [registering] the lifecycle of events.
*   **Mechanism:** It regesters [registers] the start timestamp and end time stamp [timestamp] of events.
*   **ğŸ’¡ Examples:**
    *   **ğŸŒ Distributed Tracing**
        *   Jaeger
        *   Zipkin
    *   **ğŸ’» Syscall Tracing**
        *   strace
*   **ğŸ”„ Difference from Sampling:**
    *   **Sampling:** Tells you that function A took *x* ms.
    *   **Tracing:** Tells you the specific start and end timestamp.

---

## ğŸ® Emulation
*   **Definition:** The process of running code/program on simulated hardware.
*   **ğŸ“± Common In:** 
    *   Mobile development.
    *   Embeded [Embedded] systems.
*   **âš–ï¸ Logic vs. Performance:** Emulation is great in checking logic correcteness [correctness], but not in performace [performance] measuring.
*   **âŒ Risk:** Using emulation in profiling results an inaccurate results [results in inaccurate results], because of the dirfferent [different] environments.
*   **âœ… Best Practice:** Always profile performance on real devices.

---

## âš–ï¸ Deterministic vs. ğŸ² Non-deterministic Profiling

### âš–ï¸ Deterministic Profiling
*   **Definition:** Profiling results that doesn't [don't] change.
*   **Consistency:** Each time you measure it, you get the same results.
*   **ğŸ’¡ e.g.:** Function A calls number [Function A call counts].

### ğŸ² Non-deterministic Profiling
*   **Definition:** Profiling results that always change.
*   **Consistency:** Each time you measure it, you get a different result.
*   **ğŸ’¡ e.g.:** CPU usage while running a specific function.
*   **ğŸ¯ Purpose:** Used in getting insights.

---

## ğŸ•™ Wall-clock Time vs. ğŸ’» CPU Time

### ğŸ’» CPU Time
*   **Definition:** The amount of time the processor spent actually executing instructions for your code.
*   **ğŸ› ï¸ Optimization:** If CPU time is high, you need to optimize your algorithms.

### ğŸ•™ Wall-clock Time
*   **Definition:** The total time elapsed from the start of a function to the end.
*   **ğŸ§® Formula:** Wall Time = CPU Time + Wait Time.
    *   **â³ Wait Time:** Time spent sleeping, waiting for a Database response, waiting for a file to read, or waiting for a Lock/Mutex.
*   **âš ï¸ Insight:** [If wait time is high], optimizing the code loop won't help; you need to fix the database or network.