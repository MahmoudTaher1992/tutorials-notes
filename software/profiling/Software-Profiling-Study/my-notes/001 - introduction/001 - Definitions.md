# ğŸ“‹ Definitions

## ğŸ“Š Monitoring
*   **â“ Question:** *Is it healthy?*
*   **ğŸ—ï¸ Basis:** Depends on metrics.
*   **ğŸ“‚ Data:** Metrics are data collected from the system in a time-series format.
*   **ğŸ“¤ Output:**
    *   **ğŸ“ˆ Graphs**
        *   **âœ… Benefits:** Quick to read; shows peaks and troughs.
        *   **âŒ Limitation:** Tells you there is a problem (and when it happened), but not the cause (what it is).
    *   **ğŸ”” Alerting**
        *   **âš™ï¸ Process:** Logic evaluates the graph $\rightarrow$ triggers alarms $\rightarrow$ triggers action.
        *   **ğŸ’¡ Examples:** Sending notifications or starting auto-scaling.

---

## ğŸ” Traditional Profiling 
*(Non-OpenTelemetry Profiling)*
*   **â“ Question:** *Why is it slow?*
*   **âš™ï¸ Function:** Provides detailed behavior of compiled code and resource usage.
*   **ğŸ“Š Data Examples:**
    *   **ğŸ“œ Stack traces:** Shows the sequence of function execution (especially during errors).
    *   **ğŸ§  Memory allocation:** Tracks how memory is used.
    *   **âš¡ CPU analysis:** Analyzes instruction execution.
*   **ğŸ¯ Goal:** Identify where resources are consumed and where bottlenecks exist.
*   **ğŸ©º Analogy:** It is like an **MRI scan**.

---

## ğŸª² Debugging
*   **â“ Question:** *What is the problem?*
*   **ğŸ¯ Focus:** Focuses on the logic and the flow of the code.
*   **ğŸ“‚ Data:** Variable states, breakpoints, and step-by-step execution.
*   **ğŸ Goal:** Determine the cause of the problem to propose and execute a solution.

---

## âš›ï¸ Observer Effect
*   **ğŸ“œ Origin:** This concept is from Heisenberg's Uncertainty Principle.
    *   The act of measuring a system changes the system.
*   **âš ï¸ Overhead:**
    *   Running programs/codes/scripts will consume CPU and Memory $\rightarrow$ overhead.
    *   Measuring the performance introduces an overhead.
*   **ğŸ“‰ Distortion:**
    *   The introduced overhead will distort the original results that we would have expected if the profiling tools introduced no overhead.

---

## ğŸ“ Metric Types
*   **â±ï¸ Latency**
    *   Time taken to complete something.
*   **ğŸ“¦ Throughput**
    *   Amount of work done in a duration.
*   **ğŸ”‹ Utilization**
    *   How busy a resource is (0% to 100%).
*   **ğŸš« Saturation**
    *   It is the resource state when it is fully utilized (100%) and additional requests must wait in a queue.

---

## âš–ï¸ Percentiles and Averages
*   **ğŸš¨ The Golden Rule of Profiling:** Never rely solely on Averages.
*   **âŒ Average's problem:**
    *   **Scenario:** 99 requests take 1ms. 1 request takes 10 seconds.
    *   **The Result:** The Average is ~100ms $\rightarrow$ looks okay.
    *   **The Reality:**
        *   1% of your users are angry.
        *   The average hides the outlier.
*   **ğŸ“Š Percentiles:**
    *   A ranking system.
    *   Tells you the position of a value compared to the rest of the data.
    *   **50th percentile (Median):**
        *   You scored better than 50% of the people in that group and worse than the other half.
        *   Your request is faster than half of the requests and slower than the other half.
    *   **90th percentile:**
        *   You scored better than 90% of the people in that group.
        *   Your request is faster that 10% of the requests and slower than 90% of the requests.

---

## ğŸ“œ Amdahlâ€™s Law
*   **ğŸš§ Definition:** A limit of how much you can optimize a system.
*   **ğŸ”— Core Principle:** The maximum optimization that can be done to the system is limited to the number of sequential parts of the code that can not be parallelized.
*   **ğŸ›‘ Limit:** You cannot optimize more than the time taken by the sequential latencies.

---

## ğŸŒ Universal Scalability Law
*   **ğŸ’¸ Coordination Cost:** There is a penalty of coordination that will be paid in case of parallelism.
*   **ğŸ“‰ Reality Check:**
    *   Running 2 CPUs instead of one will not give you a 100% increase in the performance.
    *   It may be lower, and in extreme cases, it will be 0 or even crash the original program.
*   **ğŸ“Š Efficiency Curve:** The number decreases as you add more resources, and then flattens, and sometime after that it crashes:
    *   1 CPU $\rightarrow$ serves 3 heavy users.
    *   2 CPU $\rightarrow$ serves 5 heavy users.
    *   10 CPU $\rightarrow$ serves 20 heavy users.
    *   100 CPU $\rightarrow$ serves 100 heavy users.
    *   200 CPU $\rightarrow$ serves 100 heavy users.