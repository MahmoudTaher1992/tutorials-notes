Based on the Table of Contents provided, section **012-Interoperability-OpenTelemetry / 002-Integration-with-other-tools** focuses on how Prometheus acts not as an isolated island, but as the "Metrics" pillar within a larger observability strategy (often referred to as the "Three Pillars of Observability": Metrics, Logs, and Traces).

Here is a detailed explanation of this specific section.

---

# Part XII: Interoperability & OpenTelemetry
## Section B: Integration with other tools

This section explores how to correlate Prometheus metrics with data from other monitoring domains to create a seamless debugging workflow. The goal is to move from **"What is happening?"** (Metrics) to **"Why is it happening?"** (Logs) and **"Where is it happening?"** (Traces).

### 1. Logging Integration (Loki)
Prometheus handles numerical data efficiently but cannot handle high-cardinality text data (logs). **Loki** is a log aggregation system designed to complement Prometheus perfectly.

#### The "Prometheus for Logs" Philosophy
Loki does not index the full text of logs (unlike Elasticsearch/Splunk); instead, it only indexes the **labels**â€”the same metadata model Prometheus uses.

*   **Shared Labeling Strategy:** The key to integration is ensuring your metrics and logs share the same labels.
    *   **Prometheus Metric:** `http_requests_total{service="payment-api", env="prod", status="500"}`
    *   **Loki Log Stream:** `{service="payment-api", env="prod"} |= "error"`
*   **The Workflow (Context Switching):**
    1.  **Spot the anomaly:** You see a spike in 500 errors in a Prometheus graph on Grafana.
    2.  **Pivot to Logs:** Because the labels (`service`, `env`, `pod`) are identical in both systems, visualization tools (like Grafana) can instantly "Split View" to show the logs generated by *that specific pod* at *that specific timestamp*.
    3.  **No New Queries:** You do not need to rewrite a search query; the labels carry the context over.

### 2. Tracing Integration (Tempo / Jaeger)
While Metrics give you the aggregate view ("Latency is high") and Logs give you the error message ("Database timeout"), **Tracing** tells you the journey of a request through distributed microservices.

#### The "Exemplar" Concept (The Glue)
Historically, linking a specific spike in a Prometheus graph to a specific trace in Jaeger was difficult. You had to guess the timestamp and manually search for traces.

Prometheus solved this with **Exemplars** (introduced in OpenMetrics).

*   **What is an Exemplar?**
    An Exemplar is a specific trace ID attached to a metric bucket. It tells Prometheus: "Here is one specific example of a request that fell into this bucket."
    *   *Scenario:* A request takes 2.5 seconds.
    *   *Metric Update:* The request count is incremented in the `le="5.0"` histogram bucket.
    *   *Exemplar Attachment:* The instrumentation library attaches `TraceID=12345` to that increment operation.

*   **How it works technically:**
    When you scrape the `/metrics` endpoint, it looks like this:
    ```text
    # HINT: This is OpenMetrics format
    http_request_duration_seconds_bucket{le="5.0"} 15 # {trace_id="12a3b45c"} 1.2
    ```
    Prometheus stores this `trace_id` alongside the data point in the Time Series Database (TSDB).

#### The Correlation Workflow
1.  **The Heatmap:** You look at a Histogram Heatmap in Grafana visualizing `http_request_duration_seconds`.
2.  **The Dots:** You see "dots" overlaying the heatmap. These are the Exemplars.
3.  **The Click:** You click on a dot representing a 5-second latency.
4.  **The Trace:** The UI immediately opens **Tempo** or **Jaeger**, querying that specific `TraceID`. You now see the waterfall chart of that exact request, revealing that the SQL query in the backend took 4.9 seconds.

### 3. The Unified Observability Experience (Summary)
This section of the study teaches that Prometheus is strongest when used as the entry point for triage:

1.  **Alert (Prometheus):** "High Error Rate detected on Payment Service."
2.  **Triage (Prometheus):** You look at the graph; it's happening on `instance="node-03"`.
3.  **Correlation (Loki):** You check logs for `instance="node-03"` and see "Connection Refused."
4.  **Root Cause (Tempo via Exemplars):** You click a slow request exemplar and see the trace failing at the "User Database" span.

**In essence, this section moves the student from knowing "how to write a query" to knowing "how to debug a system."**
