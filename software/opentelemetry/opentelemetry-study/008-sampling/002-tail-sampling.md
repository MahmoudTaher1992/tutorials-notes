Here is a detailed deep dive into **Part VIII, Section B: Tail Sampling**, formatted as the specific study guide file you requested.

***

# 008-Sampling / 002-Tail-Sampling.md

## Tail Sampling (Collector-Side)

In the previous section (Head Sampling), the decision to sample a trace was made at the **beginning** of the request (the "Head"). While efficient, Head Sampling suffers from a major flaw: it is blind to the outcome. It might keep a "boring" successful request and drop a critical "500 Internal Server Error" simply because of the roll of the dice.

**Tail Sampling** solves this by making the sampling decision at the **end** of the workflow (the "Tail"), after the entire trace (or most of it) has been generated and collected.

### 1. The Core Concept: "Wait and Decide"

Tail sampling moves the decision-making responsibility from the Application (SDK) to the **OpenTelemetry Collector**.

1.  **Ingestion:** The Collector receives all spans from all services.
2.  **Buffering:** Instead of immediately exporting these spans to a backend (like Jaeger or Datadog), the Collector holds them in memory.
3.  **Evaluation:** It waits for a specific duration or for the trace to complete.
4.  **Decision:** It inspects the full trace against a set of policies (e.g., "Did an error occur?", "Was it slow?").
5.  **Action:** If the trace matches a policy, it is exported. If not, it is dropped.

### 2. Sampling Policies

The `tail_sampling` processor in the OpenTelemetry Collector (available in the Contrib distribution) supports several policies that determine which traces are kept.

#### A. Status Code Policy (Error Sampling)
This is the most common use case. You can configure the collector to keep **100% of traces that contain an error**.
*   **Logic:** Look at all spans in a trace. If any span has `Status = Error`, keep the whole trace.
*   **Benefit:** You never miss a failed transaction, even if your overall sampling rate is low.

#### B. Latency Policy
This allows you to capture performance outliers.
*   **Logic:** Calculate the duration of the trace (Root Span duration). If `duration > threshold` (e.g., 1000ms), keep the trace.
*   **Benefit:** Allows you to debug "the slowest 1% of requests" without flooding your backend with fast, successful requests.

#### C. Probabilistic Policy
This is usually used as a fallback mechanism.
*   **Logic:** For traces that aren't errors and aren't slow, keep a random percentage (e.g., 1%).
*   **Benefit:** Provides a baseline of "normal" traffic for statistical comparison.

#### D. String/Attribute Policy
*   **Logic:** Keep traces where a specific attribute matches a value (e.g., `http.target` contains `/checkout` or `user.tier` equals `vip`).
*   **Benefit:** Ensures high-priority business transactions are always observed.

#### E. Composite Policy (The Logic Chain)
In production, you rarely use just one policy. You use a **Composite** policy to combine them using `AND` / `OR` logic.
*   *Example Goal:* "Keep all errors, keep all requests slower than 2s, AND keep 5% of everything else."

### 3. Architecture Requirements (The "Split Brain" Problem)

Implementing Tail Sampling is not as simple as adding a few lines of config. It introduces a significant architectural challenge known as the **Split Brain problem**.

#### The Problem
In a distributed system, spans for a single trace (TraceID: `ABC`) are generated by multiple services (Service A, Service B, Service C).
If you have multiple replicas of the OpenTelemetry Collector running behind a standard Round-Robin Load Balancer:
*   Service A sends spans for Trace `ABC` to **Collector Instance 1**.
*   Service B sends spans for Trace `ABC` to **Collector Instance 2**.

**Collector 1** only sees half the trace. **Collector 2** sees the other half. Neither instance has the full context required to make a correct Tail Sampling decision.

#### The Solution: Two-Layer Architecture & Load Balancing Exporter
To fix this, you must construct a two-tier collector pipeline:

1.  **Layer 1 (Agents/Gateways):** A collection layer close to the application. This layer does **not** sample. Its job is to forward data.
2.  **The Load Balancing Exporter:** Layer 1 uses a `load_balancing` exporter. This exporter hashes the `TraceID`.
    *   *Result:* All spans with TraceID `ABC` are mathematically guaranteed to be sent to **Collector Instance 3** in the next layer.
3.  **Layer 2 (The Sampling Tier):** This tier receives *all* spans for a specific trace into a single box. It buffers the data, runs the Tail Sampling logic, and exports to the backend.

### 4. Configuration Example (`config.yaml`)

Here is what the configuration looks like on the **Layer 2** collector:

```yaml
processors:
  tail_sampling:
    decision_wait: 10s   # Wait 10s for all spans to arrive
    num_traces: 50000    # Keep up to 50k traces in memory at once
    expected_new_traces_per_sec: 500
    policies:
      [
        {
          name: keep-errors,
          type: status_code,
          status_code: { status_codes: [ERROR] }
        },
        {
          name: keep-slow-requests,
          type: latency,
          latency: { threshold_ms: 2000 }
        },
        {
          name: randomize-success,
          type: probabilistic,
          probabilistic: { sampling_percentage: 1 }
        }
      ]

exporters:
  otlp:
    endpoint: "tempo:4317"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [tail_sampling, batch]
      exporters: [otlp]
```

### 5. Pros and Cons of Tail Sampling

| Feature | Head Sampling (SDK) | Tail Sampling (Collector) |
| :--- | :--- | :--- |
| **Completeness** | Low. Relies on chance. | High. Can capture 100% of "interesting" data. |
| **Resource Cost** | Low. Dropped data is never processed. | High. **All** data must be generated, sent, and buffered in RAM before a decision is made. |
| **Bandwidth** | Saves network bandwidth from App to Collector. | Consumes full network bandwidth from App to Collector. |
| **Complexity** | Simple configuration. | High. Requires specific Load Balancing architecture. |
| **Delay** | None. | Introduced latency (equal to `decision_wait` time) before data appears in dashboards. |

### Summary
Tail Sampling is the "Gold Standard" for observability because it maximizes signal (errors/latency) while minimizing noise (repetitive success logs). However, it requires investing in a more robust OpenTelemetry Collector infrastructure with high memory capacity and sticky load balancing.